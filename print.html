<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Machine learning system design patterns</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded "><a href="training_workflow.html"><strong aria-hidden="true">1.</strong> Training workflow</a></li><li class="chapter-item expanded "><a href="decouple_feature_creation.html"><strong aria-hidden="true">2.</strong> Decoupling feature creation from workflows</a></li><li class="chapter-item expanded "><a href="embedd_processing_logic_model.html"><strong aria-hidden="true">3.</strong> Embed pre & post processing logic in the model</a></li><li class="chapter-item expanded "><a href="data_validation_inference.html"><strong aria-hidden="true">4.</strong> Data validation for inference</a></li><li class="chapter-item expanded "><a href="online_serving_structure.html"><strong aria-hidden="true">5.</strong> Online serving structure</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Machine learning system design patterns</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>When I started working in the field of machine learning engineering many years ago the process of designing machine learning systems seemed like black magic since there weren't a lot of public material on the subject. </p>
<p>However, last couple of years several good books on machine learning system design has been published, such as <a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/">Designing Machine Learning Systems</a> and <a href="https://www.oreilly.com/library/view/machine-learning-design/9781098115777/">Machine Learning Design Patterns</a>. This e-book is complementary to these, providing additional design patterns that I have found useful during my career when designing machine learning systems.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="training-workflow"><a class="header" href="#training-workflow">Training workflow</a></h1>
<p>A common structure for a training pipeline is visualized below:</p>
<pre class="mermaid">graph LR
    A[Data ingestion] --&gt; B[Data validation]
    B --&gt; C[Feature engineering]
    C --&gt; D[Train model]
    D --&gt; E[Model analysis]
</pre>
<p>I first saw this structure in the <a href="https://www.tensorflow.org/tfx/guide">TFX project</a>, but it might originate somewhere else. We could simply use Tensorflow Extended (TFX) for our training workflow or implement this structure in another framework, such as <a href="https://github.com/kubeflow/pipelines">Kubeflow Pipelines</a>.</p>
<h2 id="data-ingestion"><a class="header" href="#data-ingestion">Data ingestion</a></h2>
<p>In this task we load the data needed for training our machine learning model from an external system, e.g. a feature-store or a data warehouse. Typically in this step we also split the data into separate sets: training, validation and test.</p>
<h2 id="data-validation"><a class="header" href="#data-validation">Data validation</a></h2>
<p>In this task we will validate that our data follows some pre-defined structure. One common case is that the data follows a given schema. For example, we might expects that:</p>
<ul>
<li>That the column 'prices' should be of type float and should not be negative.</li>
<li>That the column 'country' should be of type string and can only have the following values: ['US', 'CA', 'MX']</li>
</ul>
<p>During this step we can also compute statistics about the data, such as histogram of numerical columns. These statics will be useful during serving when are trying to detection training-serving skews.</p>
<p>We can also compare the distribution between the training set and the validation &amp; test set. This can be useful to detection any issues creating when splitting the data. </p>
<p>For example, image that we decide to train on data from users in USA and evaluate on data from users in Canada and Mexico. However, the behaviour of users in USA might be completely different from users in Canada and Mexico and therefore our model might perform poorly on the validation set. Comparing the distribution between training and validation can give us useful information how we need to change our splitting strategy.</p>
<h2 id="feature-engineering"><a class="header" href="#feature-engineering">Feature engineering</a></h2>
<p>In this task we will do the feature creation and the feature pre-processing.</p>
<p>By feature creation I mean constructing the actual features. For example, calculating a rolling average of bought items for a given users during the last 14 days. Note that in general we would like to do this outside of the training workflow, see <a href="decouple_feature_creation.html">decoupling feature creation from training &amp; serving workflow</a>.</p>
<p>However there might be special cases where we have to do it in the training workflows. As an example, image that as part of the inference request we get a list of outlier days that the user has provided and we have a rolling average feature. In this case, we need to calculate the rolling average after removing the outliers, otherwise they will influence this feature. Since this feature needs to be calculated at &quot;runtime&quot; for inference it makes sense to do the same for training, allowing us to re-use the functionality and avoid skews.</p>
<p>But in general this should be avoided if possible, as described in <a href="decouple_feature_creation.html">decoupling feature creation from training &amp; serving workflow</a>. And sometimes we could re-frame the feature to avoid having to compute it at &quot;runtime&quot;. In the example above, we could potentially use median instead of mean to remove the need for the user to provide the outlier days.</p>
<p>The second part is feature pre-processing. Here we transform the features into a more desirable format. For example, we could transform a column of words to integers through tokenization, Z-normalize a numerical column etc. A common pattern is to include this logic in the model, see <a href="embedd_processing_logic_model.html">embed pre &amp; post processing logic in the model</a>.</p>
<p>However, we might do the pre-processing in this task and then embed this into the model at a later time. The reason why we might want to do that is because this processing can be computationally heavy, especially if there is a large amount of data. This is actually what <a href="https://www.tensorflow.org/tfx/transform/get_started">Tensorflow transform</a> does.</p>
<h2 id="train-model"><a class="header" href="#train-model">Train model</a></h2>
<p>In this step we have assembled everything needed to start training our machine learning model. To start with, me might begin with hyperparameter tuning where we try to find a good combination of hyper-parameters (e.g. learning-rate, batch-size etc).</p>
<p>Once those has been discovered we can do our final training with the &quot;best&quot; hyper-parameters we found.</p>
<h2 id="model-analysis"><a class="header" href="#model-analysis">Model analysis</a></h2>
<p>The final step in our workflow is the model analysis. Here we will inspect the models performance, such as accuracy, F1-score, root-mean squared error etc. Furthermore, it's common to check the performance on different slices of the data to find potential imbalances in performance between the slices.</p>
<p>For example, let's say our data comes from users in two different regions: US and EU. Now it can be valuable to check what the accuracy of the model is for users in US and EU. If there is a large discrepancy between these two groups it can flag that we might have an issue with the model. As an example, the average accuracy is good for the two groups, but the model performs much worse for users in EU then in US. <a href="https://www.tensorflow.org/tfx/model_analysis/get_started">TensorFlow Model Analysis</a> is a tool that helps us do this model analysis.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="decoupling-feature-creation-from-training--serving-workflow"><a class="header" href="#decoupling-feature-creation-from-training--serving-workflow">Decoupling feature creation from training &amp; serving workflow</a></h1>
<p>This design pattern aims to decouple the feature creation from the training and serving workflows. </p>
<p>It allows for separation of concerns; it let people with expertise in data engineering to focus creating scalable and robust feature creation pipelines while experts in model builders can focus on the modelling parts.</p>
<p>Decrease the risk of training-serving skews. A common cause of bugs in machine learning systems are difference in data processing between training and serving. </p>
<p>For example, imagine that we are building a machine learning system for recommending products to users. During our modelling phase we have discovered that the price is an important feature. However, accidentally in our training pipeline we have defined the price in American dollars (USD), while in the serving workflow we have defined it in the native currency (i.e. for Swedish users we define it in Swedish Krona). This would not necessarily break our system but our predictions would most be worse then if we had implemented both in the same currency, e.g. USD.</p>
<p>This might seem like a obvious example, but these types of issues crop up all the time. I have seen that simply unifying the feature creation between training and serving workflows substantially increase the accuracy of system.</p>
<p>A common term within machine learning systems are <a href="https://www.tecton.ai/blog/what-is-a-feature-store/">feature stores</a>. Does that mean we always need to include a feature store in our systems? Not necessarily. </p>
<p>For example, imagine that we have the recommendation problem where we will pre-compute the recommendation before hand, i.e. doing batch inference a head of time and store the predictions in a database. When we want to serve these recommendations, we simply do a lookup in our database. Furthermore, assume that the cardinality of users &amp; products are not too big. Then for our feature creation we could simply store these features in a data warehouse / datalake, where each row contains all the feature for users, articles, the the combination of feature &amp; users.</p>
<p>Below is an example table with this structure:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: center">article_id</th><th style="text-align: center">user_id</th><th style="text-align: center">article_type</th><th style="text-align: center">user_previous_purchases</th><th style="text-align: center">user_article_affinity</th></tr></thead><tbody>
<tr><td style="text-align: center">1582</td><td style="text-align: center">5768</td><td style="text-align: center">electronics</td><td style="text-align: center">192.0</td><td style="text-align: center">0.8</td></tr>
<tr><td style="text-align: center">1582</td><td style="text-align: center">9922</td><td style="text-align: center">electronics</td><td style="text-align: center">29.0</td><td style="text-align: center">0.3</td></tr>
<tr><td style="text-align: center">1988</td><td style="text-align: center">9922</td><td style="text-align: center">clothes</td><td style="text-align: center">29.0</td><td style="text-align: center">0.1</td></tr>
</tbody></table>
</div>
<p>Whether we should implement this ourself like above or use an external platform like <a href="https://docs.feast.dev/">Feast</a> is context dependent. The above approach is good since we don't have to learn a new tool and integrate with it. We can simply use our preferred data warehouse / datalake solution.</p>
<p>However, if we instead would like to do online inference the approach above has most likely too high latency for feature retrieval. To improve the latency of retrieval, we could store the features for inference in a key-value store, like Redis. However, adding these capabilities would require a lot of engineering efforts and simply using a third-party system like Feast is most likely easier.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="embed-pre--post-processing-logic-in-the-model"><a class="header" href="#embed-pre--post-processing-logic-in-the-model">Embed pre &amp; post processing logic in the model</a></h1>
<p>Before we feed the data into our machine learning model we usually have pre-processing steps. For example, we could transform a column of words to integers through tokenization, Z-normalize a numerical column etc. It's very important that these steps gets applied in the same way for training as in serving.</p>
<p>For example, assume we have a feature thats follows a normal distribution with mean 5 and standard deviation of 2. Before feeding this column into our machine learning model (say a decision tree) we Z-normalize the column to mean 0 and standard deviation of 1. Now, during training of our decision tree we have made a split, s.t. if the value is smaller then 0.5 then we take the left branch, otherwise the right. Now if we forget to apply this Z-normalization on the feature almost all of the data points will be larger 0.5, causing the model to behave completely differently.</p>
<p>So how do we make sure that the pre-processing steps gets applied in a similar fashion between training and serving? A common pattern is to embed this step into the model object. Now when we save the model after training in order to be used for serving, this step gets automatically included. An example of this approach is <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">Sklearn pipelines</a>. Below is an example of a Sklearn pipeline where we Z-normalize the feature &quot;revenue&quot; as part of our pipeline:</p>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression

num_cols = [&quot;revenue&quot;]

numerical_pipeline = Pipeline(steps=[
    (&quot;Z-normalize&quot;, StandardScaler())
])

col_transform = ColumnTransformer(transformers=[
        (&quot;num_pipeline&quot;, numerical_pipeline, num_cols),
    ]
)

model = LinearRegression()

model_pipeline = Pipeline(steps=[
    (&quot;col_trans&quot;, col_transform),
    (&quot;model&quot;, model)
])

# The serialized model will contain the z-normalize step
with open(&quot;model.pkl&quot;,&quot;wb&quot;) as fp:
    pickle.dump(model_pipeline, fp)
</code></pre>
<p>Frameworks such as Tensorflow and PyTorch also have similar support for this pattern. In this case, we will embed the pre-processing steps into the computational graph. Then when we save the model these steps will be included automatically, since it is a part of computational graph.</p>
<p>When we have a large amount of data for training it can be a good idea to do the pre-processing outside of the model (preferably with a distributed computing framework) and later embed the pre-processing into the model object before we save it for serving. This is actually what <a href="https://www.tensorflow.org/tfx/transform/get_started">Tensorflow transform</a> does. Here the pre-processing step is done with Apache Beam and once we have finished training we embed these steps into the computational graph which gets exported.</p>
<p>This pattern can also be applied to the post-processing step. For example, imagine that we want to make sure our predictions are non-negative. We could add a post-processing step where we take the maximum of our predictions and 0. To make sure that this gets applied in a similar fashion in both training and serving we should embed this step into our model object, as we did for pre-processing.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-validation-for-inference"><a class="header" href="#data-validation-for-inference">Data validation for inference</a></h1>
<p>During our training workflow we produce the data validation artifact which describes the data schema and the dataset distribution, see <a href="training_workflow.html">training workflow</a>. Now during serving we want to validate that our serving data has the same data schema and comes from a similar distribution to our training data in order to avoid training-serving skew. Comparing distributions can be done with measures such as <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence">Jensen–Shannon divergence</a>.</p>
<p>Below we describe how to do this validation for batch &amp; online inference.</p>
<h2 id="batch-inference"><a class="header" href="#batch-inference">Batch inference</a></h2>
<p>A typical batch-inference workflow has the following structure:</p>
<pre class="mermaid">graph LR
    A[Data ingestion] --&gt; B[Data validation]
    B --&gt; C[Model inference]
</pre>
<p>In the data validation task we will perform the validation. That is, compare the data schema for the ingested data for inference and the produced data schema during training. We will also compare the data distribution between the two datasets. If the schemas &amp; data distributions are the same we will continue to the model inference task, otherwise we might fail the workflow and/or notify the developers that something is wrong.</p>
<h2 id="online-inference"><a class="header" href="#online-inference">Online inference</a></h2>
<p>Doing the data validation for online inference is a bit trickier. First of, most metrics to detect distribution differences requires a sizeable amount of data. For online inference we might only have at hand one or a couple of data-points (i.e. batched requests) at hand and computing this difference is not meaningful. Also doing this data validation would introduce more latency to our system since it's an additional step we need to perform.</p>
<p>So how can we solve this? One approach is take the requests we get in the online inference service and forward them to an additional service, called data validation service. Here we will store the requests and once we have reached an adequate amount of data we will trigger a batch data validation job where we compare the data schema and distribution for these requests with the ones produced by the training workflow. If the schema &amp; data distribution are different, we will notify the developers that something is wrong. We could also run this as a cron job such that we do the data validation on new requests every X hours or days.</p>
<p>Furthermore, if the amount of requests is large we could sample these requests randomly such that we only do the data validation on a percentage of the requests.</p>
<p>Below is an overview of this architecture:</p>
<pre class="mermaid">graph LR
    A[User] &lt;--&gt; B[Online inference service]
    B --&gt; C[Data validation service]
</pre>
<h2 id="distribution-skew-versus-drift"><a class="header" href="#distribution-skew-versus-drift">Distribution skew versus drift</a></h2>
<p>Training-serving skew occurs when the data distributions for training and serving is different from the beginning. As an example, imaging that one of our input feature is the price of a product. During training, we have assumed that the price is in Euros. However, during serving we accidentally input prices in USD, which causes the price feature to have different distributions between training and serving.</p>
<p>Sometimes however the distributions are the same in the begging but drifts apart over time. Imagine the price feature described above. Now instead assume that the price is in Euros for both training and serving. But due to inflation, the prices increases over time. Therefore, the distributions drifts apart, causing model performance to degrade. This is called a distribution drift.</p>
<h2 id="metrics-to-determine-distribution-skew--drift"><a class="header" href="#metrics-to-determine-distribution-skew--drift">Metrics to determine distribution skew / drift</a></h2>
<p>In a supervised learning setting, we are typically trying to estimate \( P(Y|X) \) where \( X \) is our features and \( Y \) is the target. That is, we are trying to estimate the probability distribution of the target variable, given our features. For a classification problem, we get the discrete probability distribution over the class given a feature input.</p>
<p>Using Bayes' theorem we get that:</p>
<p>\[ P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)} \]</p>
<p>Now, if any of the probabilities \( P(Y|X) \), \( P(X|Y) \), \( P(Y) \) or \( P(X) \) changes between training and serving the model performance usually degrades. Since \( P(X|Y) \) is neither estimated during training or observed directly it's usually skipped for monitoring distribution drifts. So during serving we cam compare:</p>
<ul>
<li>\( P(X) \), called covariance drift.</li>
<li>\( P(Y)\) , called label drift.</li>
<li>\( P(Y|X) \), called concept drift.</li>
</ul>
<p>The difference in \( P(X) \) between training and serving can always be compared since we observe the features before being send into our model.</p>
<p>\( P(Y) \) and \( P(Y|X) \) is a bit more tricky to observe potentially. This is because we need access to the ground truth of the target values, which is not always possible to obtain. This usually depends on the task at hand. Imaging the following two systems:</p>
<p>A machine learning system that is trying to predict whether a user will click on an ad or not. Now in our internal system, we will most likely keep track of whether the user clicked on it or not. Therefore we can estimate \( P(Y) \) and \( P(Y|X) \) and compare it between training and serving.</p>
<p>A machine learning system trying to predict whether an image is a dog or a cat. Now unless the user gives us feedback on if the image was correctly labelled or not, we cannot determine the ground truth. Therefore, estimating \( P(Y) \) and \( P(Y|X) \) during serving will be impossible, unless we label the incoming images ourself.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="online-serving-structure"><a class="header" href="#online-serving-structure">Online serving structure</a></h1>
<p>When deploying our machine learning model for online inference we typically divide into two parts, our backend service and our machine learning inference service. Below is a diagram of the high-level overview of this setup:</p>
<pre class="mermaid">graph LR
    A[User] &lt;--&gt; B[Backend service]
    B &lt;--&gt; C[Machine learning inference service]
</pre>
<p>Our backend service is responsible authenticate the request, fetching necessary data from the feature store etc. The machine learning inference service will be tasked with doing the actual inference of the machine learning model. It can be broken up into three parts; pre-process, predict and post-process. </p>
<p>The pre-process step include steps like validating that the required data is available for doing the request, that the data types are correct etc. We try to avoid including model specific pre-processing step here, see <a href="embedd_processing_logic_model.html">Embed pre &amp; post processing logic in the model</a> for clarification.</p>
<p>The predict part is where we do the actual model inference. We send in the data produces from pre-process step into the model to get the predictions.</p>
<p>Lastly, we have the post-process step. Here we transform the outputted predictions into a format which can be used by the system as a whole. For example, we might transformed predicted class-ids into class names, such as 0 into the name 'Dog'.</p>
<p>Now with these tree steps, how should they be deployed? As one service or spread out into multiple? There is a few different approaches for doing this which we will delve into now with their own advantages and disadvantages.</p>
<h2 id="group-all-steps-together-in-one-service"><a class="header" href="#group-all-steps-together-in-one-service">Group all steps together in one service</a></h2>
<p>The first approach we will discuss is to group all the steps into one service. Below is an example code structure:</p>
<pre><code class="language-python">class InferenceHandler:
    def __init__(self, model_path):
        # load the model, e.g. with pickle
        pass

    def pre_process(self, x):
        # define the pre-process logic in this function
        pass

    def predict(self, x):
        # define how the model's predict function should be called in this function
        pass

    def post_process(self, x):
        # define the post-process logic in this function
        pass

    def infer(self, x):
        # in this function we glue all of the above step together
        return self.post_process(self.predict(self.pre_process(x)))
</code></pre>
<p>The class above would then be included in a web-server which exposes an end-point that allow users to call it through HTTP or gRPC.</p>
<p>The advantages of this approach that it is simple. You only need to deploy one service, making the deployment process a lot smoother. Furthermore, keeping the different steps in synchronisation is a lot easier. For example, imaging we wanted to update our predict function to accept batches of inputs (i.e. from input shape of (num_features) to (num_samples, num_features)). This change would require us to also update the pre &amp; post-processing steps. Now, if these were deployed separately, we would need re-deploy all the services which is more cumbersome.</p>
<p><a href="https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/prediction/sklearn/predictor.py">VertexAI model serving</a> and <a href="https://docs.ray.io/en/latest/serve/index.html">Ray Serve</a> are examples of frameworks that uses this approach.</p>
<h2 id="separate-each-step-into-a-service"><a class="header" href="#separate-each-step-into-a-service">Separate each step into a service</a></h2>
<p>The next approach is to separate each step into a service. These would then communicate over HTTP / gRPC. Below is a diagram of the communication flow:</p>
<pre class="mermaid">graph LR
    A[External service] --&gt; B[Pre-process service]
    B --&gt; C[Predict service]
    C --&gt; D[Post-process service]
    D --&gt; A
</pre>
<p>Now you might ask &quot;why would we want to do this&quot;? To answer that question, lets imagine the following case-study:</p>
<p>Let's say we have a large convolutional neural network that we want to deploy. To speed up the inference part we decide to run the model on a GPU. Furthermore, our pre-processing step is quite time-consuming, e.g. we want crop the image and convert from <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a> to <a href="https://en.wikipedia.org/wiki/HSL_and_HSV">HSV</a>. Moreover, let's say we are getting an increase in request so we need to scale-up this solution. If all the steps are grouped together, we need deploy additional replicas of the same service, regardless of which part is the bottleneck. Furthermore, during the pre &amp; post-processing step our GPU is idle which is wasteful since GPU's are expensive. Now imagine these were separate steps, then we could scale them separately depending on which part was the bottleneck.</p>
<p>Another advantage of this separation is that we can co-locate our models on a model inference server, such as <a href="https://github.com/triton-inference-server/server">triton-inference-server</a> and <a href="https://github.com/tensorflow/serving">tensorflow serving</a>. Now we could load multiple replicas of a model and/or multiple models. This is advantages since it allow us to utilize our GPU better.</p>
<p>Lastly, most of these inference servers allows us to off-load models that haven't been used in a while, and load it once a request comes for that specific model (of course with a latency cost). This feature can be helpful if we need to serve a large number of models. For example, imagine that we have a model that forecasts how much we will sell of an item in a given country. Through our experiments we have determined that having one model per country performed better then a global one. Furthermore, assume that we operate in many different countries. If we have included all steps in one service, we would have to deploy one of these services for each country (and potentially more if the number of requests are large). However when the steps are separated, we could co-locate all of the models on an inference server, potentially getting away with fewer deployments.</p>
<p><a href="https://github.com/kserve/kserve">Kserve</a> and <a href="https://github.com/SeldonIO/seldon-core">Seldon</a> are examples of frameworks that uses this approach.</p>
<h2 id="when-to-use-which-approach"><a class="header" href="#when-to-use-which-approach">When to use which approach?</a></h2>
<p>So now naturally the question arises: &quot;Which of these approaches should I take?&quot;</p>
<p>I would choose &quot;grouping all steps together&quot; if the following condition were meet:</p>
<ul>
<li>Number of served models are small for the application</li>
<li>The model doesn't require an accelerator, such as GPUs, TPUs etc</li>
</ul>
<p>Otherwise I would go &quot;separate each step&quot;.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid.min.js"></script>
        <script src="mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
